# -*- coding: utf-8 -*-
"""PSNR_SSIM_for_denoior_calculate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O7ZgQaOX--zsHwly57kY96xMIZKk4cgz
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
from pathlib import Path

def add_gaussian_noise(image, mean=0, std=25):
    """
    Adds Gaussian noise to an image.

    Args:
        image (numpy.ndarray): The input image.
        mean (float): Mean of the Gaussian noise.
        std (float): Standard deviation of the Gaussian noise.

    Returns:
        numpy.ndarray: The image with Gaussian noise.
    """
    gauss = np.random.normal(mean, std, image.shape).astype('float32')
    noisy_image = image.astype('float32') + gauss
    noisy_image = np.clip(noisy_image, 0, 255)  # Ensure pixel values are valid
    return noisy_image.astype('uint8')

def process_folder(input_folder, output_folder, mean=0, std=25):
    """
    Processes all images in the input folder by adding Gaussian noise
    and saves them to the output folder, preserving the class structure.

    Args:
        input_folder (str): Path to the folder with original images.
        output_folder (str): Path to save the noisy images.
        mean (float): Mean of the Gaussian noise.
        std (float): Standard deviation of the Gaussian noise.
    """
    input_path = Path(input_folder)
    output_path = Path(output_folder)

    # Create the output folder structure
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            (output_path / class_folder.name).mkdir(parents=True, exist_ok=True)

    # Process each class and add Gaussian noise
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            for image_file in class_folder.glob('*.*'):  # Adjust extensions as needed
                # Read the image
                image = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue  # Skip unreadable files

                # Add Gaussian noise
                noisy_image = add_gaussian_noise(image, mean, std)

                # Save the noisy image
                output_file = output_path / class_folder.name / image_file.name
                cv2.imwrite(str(output_file), noisy_image)

if __name__ == "__main__":
    # Define paths
    input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b/Testing"  # Input folder
    output_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b_noisy/Testing_noisy"  # Output folder

    # Noise parameters
    mean = 0  # Mean of Gaussian noise
    std = 25  # Standard deviation of Gaussian noise

    # Process images
    process_folder(input_folder, output_folder, mean, std)

import os
import cv2
import numpy as np
from pathlib import Path

def apply_motion_blur(image, kernel_size=15):
    """
    Applies motion blur to an image.

    Args:
        image (numpy.ndarray): The input image.
        kernel_size (int): The size of the kernel for motion blur. Larger values create stronger blur.

    Returns:
        numpy.ndarray: The image with motion blur.
    """
    # Create a motion blur kernel
    kernel = np.zeros((kernel_size, kernel_size))
    kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)
    kernel /= kernel_size

    # Apply the motion blur kernel to the image
    blurred_image = cv2.filter2D(image, -1, kernel)
    return blurred_image

def process_folder(input_folder, output_folder, blur_type='motion', kernel_size=15, mean=0, std=25):
    """
    Processes all images in the input folder by applying either motion blur or Gaussian noise
    and saves them to the output folder, preserving the class structure.

    Args:
        input_folder (str): Path to the folder with original images.
        output_folder (str): Path to save the processed images.
        blur_type (str): Type of effect to apply ('motion' for motion blur, 'gaussian' for Gaussian noise).
        kernel_size (int): The size of the kernel for motion blur.
        mean (float): Mean of the Gaussian noise (if applicable).
        std (float): Standard deviation of the Gaussian noise (if applicable).
    """
    input_path = Path(input_folder)
    output_path = Path(output_folder)

    # Create the output folder structure
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            (output_path / class_folder.name).mkdir(parents=True, exist_ok=True)

    # Process each class and apply the specified effect
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            for image_file in class_folder.glob('*.*'):  # Adjust extensions as needed
                # Read the image
                image = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue  # Skip unreadable files

                if blur_type == 'motion':
                    # Apply motion blur
                    processed_image = apply_motion_blur(image, kernel_size)
                elif blur_type == 'gaussian':
                    # Apply Gaussian noise
                    processed_image = add_gaussian_noise(image, mean, std)
                else:
                    raise ValueError("Invalid blur_type. Use 'motion' or 'gaussian'.")

                # Save the processed image
                output_file = output_path / class_folder.name / image_file.name
                cv2.imwrite(str(output_file), processed_image)

if __name__ == "__main__":
    # Define paths
    input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b/Training"  # Input folder
    output_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b/Training_processed"  # Output folder

    # Effect parameters
    blur_type = 'motion'  # Choose 'motion' for motion blur or 'gaussian' for Gaussian noise
    kernel_size = 15  # Size of the motion blur kernel (only used for motion blur)
    mean = 0  # Mean of Gaussian noise (only used for Gaussian noise)
    std = 25  # Standard deviation of Gaussian noise (only used for Gaussian noise)

    # Process images
    process_folder(input_folder, output_folder, blur_type, kernel_size, mean, std)





import os
import cv2
import numpy as np
from pathlib import Path

def apply_motion_blur(image, kernel_size=15):
    """
    Applies motion blur to an image.

    Args:
        image (numpy.ndarray): The input image.
        kernel_size (int): The size of the kernel for motion blur. Larger values create stronger blur.

    Returns:
        numpy.ndarray: The image with motion blur.
    """
    # Create a motion blur kernel
    kernel = np.zeros((kernel_size, kernel_size))
    kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)
    kernel /= kernel_size

    # Apply the motion blur kernel to the image
    blurred_image = cv2.filter2D(image, -1, kernel)
    return blurred_image

def process_folder(input_folder, output_folder, blur_type='motion', kernel_size=15, mean=0, std=25):
    """
    Processes all images in the input folder by applying either motion blur or Gaussian noise
    and saves them to the output folder, preserving the class structure.

    Args:
        input_folder (str): Path to the folder with original images.
        output_folder (str): Path to save the processed images.
        blur_type (str): Type of effect to apply ('motion' for motion blur, 'gaussian' for Gaussian noise).
        kernel_size (int): The size of the kernel for motion blur.
        mean (float): Mean of the Gaussian noise (if applicable).
        std (float): Standard deviation of the Gaussian noise (if applicable).
    """
    input_path = Path(input_folder)
    output_path = Path(output_folder)

    # Create the output folder structure
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            (output_path / class_folder.name).mkdir(parents=True, exist_ok=True)

    # Process each class and apply the specified effect
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            for image_file in class_folder.glob('*.*'):  # Adjust extensions as needed
                # Read the image
                image = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue  # Skip unreadable files

                if blur_type == 'motion':
                    # Apply motion blur
                    processed_image = apply_motion_blur(image, kernel_size)
                elif blur_type == 'gaussian':
                    # Apply Gaussian noise
                    processed_image = add_gaussian_noise(image, mean, std)
                else:
                    raise ValueError("Invalid blur_type. Use 'motion' or 'gaussian'.")

                # Save the processed image
                output_file = output_path / class_folder.name / image_file.name
                cv2.imwrite(str(output_file), processed_image)

if __name__ == "__main__":
    # Define paths
    input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b/Testing"  # Input folder
    output_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b_motionblure/Testing_processed"  # Output folder

    # Effect parameters
    blur_type = 'motion'  # Choose 'motion' for motion blur or 'gaussian' for Gaussian noise
    kernel_size = 15  # Size of the motion blur kernel (only used for motion blur)
    mean = 0  # Mean of Gaussian noise (only used for Gaussian noise)
    std = 25  # Standard deviation of Gaussian noise (only used for Gaussian noise)

    # Process images
    process_folder(input_folder, output_folder, blur_type, kernel_size, mean, std)

import os
import cv2
import numpy as np
from pathlib import Path

def apply_motion_blur(image, kernel_size=15):
    """
    Applies motion blur to an image.

    Args:
        image (numpy.ndarray): The input image.
        kernel_size (int): The size of the kernel for motion blur. Larger values create stronger blur.

    Returns:
        numpy.ndarray: The image with motion blur.
    """
    # Create a motion blur kernel
    kernel = np.zeros((kernel_size, kernel_size))
    kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)
    kernel /= kernel_size

    # Apply the motion blur kernel to the image
    blurred_image = cv2.filter2D(image, -1, kernel)
    return blurred_image

def process_folder(input_folder, output_folder, blur_type='motion', kernel_size=15, mean=0, std=25):
    """
    Processes all images in the input folder by applying either motion blur or Gaussian noise
    and saves them to the output folder, preserving the class structure.

    Args:
        input_folder (str): Path to the folder with original images.
        output_folder (str): Path to save the processed images.
        blur_type (str): Type of effect to apply ('motion' for motion blur, 'gaussian' for Gaussian noise).
        kernel_size (int): The size of the kernel for motion blur.
        mean (float): Mean of the Gaussian noise (if applicable).
        std (float): Standard deviation of the Gaussian noise (if applicable).
    """
    input_path = Path(input_folder)
    output_path = Path(output_folder)

    # Create the output folder structure
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            (output_path / class_folder.name).mkdir(parents=True, exist_ok=True)

    # Process each class and apply the specified effect
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            for image_file in class_folder.glob('*.*'):  # Adjust extensions as needed
                # Read the image
                image = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue  # Skip unreadable files

                if blur_type == 'motion':
                    # Apply motion blur
                    processed_image = apply_motion_blur(image, kernel_size)
                elif blur_type == 'gaussian':
                    # Apply Gaussian noise
                    processed_image = add_gaussian_noise(image, mean, std)
                else:
                    raise ValueError("Invalid blur_type. Use 'motion' or 'gaussian'.")

                # Save the processed image
                output_file = output_path / class_folder.name / image_file.name
                cv2.imwrite(str(output_file), processed_image)

if __name__ == "__main__":
    # Define paths
    input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b/Training"  # Input folder
    output_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b_motionblure/Training_processed"  # Output folder

    # Effect parameters
    blur_type = 'motion'  # Choose 'motion' for motion blur or 'gaussian' for Gaussian noise
    kernel_size = 15  # Size of the motion blur kernel (only used for motion blur)
    mean = 0  # Mean of Gaussian noise (only used for Gaussian noise)
    std = 25  # Standard deviation of Gaussian noise (only used for Gaussian noise)

    # Process images
    process_folder(input_folder, output_folder, blur_type, kernel_size, mean, std)



import os
import cv2
import numpy as np
from pathlib import Path

def add_salt_and_pepper_noise(image, salt_prob=0.01, pepper_prob=0.01):
    """
    Adds salt-and-pepper noise to an image.

    Args:
        image (numpy.ndarray): The input image.
        salt_prob (float): Probability of adding salt noise (white pixels).
        pepper_prob (float): Probability of adding pepper noise (black pixels).

    Returns:
        numpy.ndarray: The image with salt-and-pepper noise.
    """
    noisy_image = image.copy()
    total_pixels = image.size

    # Add salt (white) noise
    num_salt = int(total_pixels * salt_prob)
    coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape]
    noisy_image[coords[0], coords[1]] = 255

    # Add pepper (black) noise
    num_pepper = int(total_pixels * pepper_prob)
    coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape]
    noisy_image[coords[0], coords[1]] = 0

    return noisy_image

def apply_motion_blur(image, kernel_size=15):
    """
    Applies motion blur to an image.

    Args:
        image (numpy.ndarray): The input image.
        kernel_size (int): The size of the kernel for motion blur. Larger values create stronger blur.

    Returns:
        numpy.ndarray: The image with motion blur.
    """
    # Create a motion blur kernel
    kernel = np.zeros((kernel_size, kernel_size))
    kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)
    kernel /= kernel_size

    # Apply the motion blur kernel to the image
    blurred_image = cv2.filter2D(image, -1, kernel)
    return blurred_image

def add_gaussian_noise(image, mean=0, std=25):
    """
    Adds Gaussian noise to an image.

    Args:
        image (numpy.ndarray): The input image.
        mean (float): Mean of the Gaussian noise.
        std (float): Standard deviation of the Gaussian noise.

    Returns:
        numpy.ndarray: The image with Gaussian noise.
    """
    gauss = np.random.normal(mean, std, image.shape).astype('float32')
    noisy_image = image.astype('float32') + gauss
    noisy_image = np.clip(noisy_image, 0, 255)  # Ensure pixel values are valid
    return noisy_image.astype('uint8')

def process_folder(input_folder, output_folder, noise_type='motion', kernel_size=15, mean=0, std=25, salt_prob=0.01, pepper_prob=0.01):
    """
    Processes all images in the input folder by applying the selected noise type
    and saves them to the output folder, preserving the class structure.

    Args:
        input_folder (str): Path to the folder with original images.
        output_folder (str): Path to save the processed images.
        noise_type (str): Type of effect to apply ('motion', 'gaussian', 'salt_pepper').
        kernel_size (int): The size of the kernel for motion blur.
        mean (float): Mean of the Gaussian noise (if applicable).
        std (float): Standard deviation of the Gaussian noise (if applicable).
        salt_prob (float): Probability of salt noise (only used for salt-and-pepper noise).
        pepper_prob (float): Probability of pepper noise (only used for salt-and-pepper noise).
    """
    input_path = Path(input_folder)
    output_path = Path(output_folder)

    # Create the output folder structure
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            (output_path / class_folder.name).mkdir(parents=True, exist_ok=True)

    # Process each class and apply the specified noise
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            for image_file in class_folder.glob('*.*'):  # Adjust extensions as needed
                # Read the image
                image = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue  # Skip unreadable files

                if noise_type == 'motion':
                    # Apply motion blur
                    processed_image = apply_motion_blur(image, kernel_size)
                elif noise_type == 'gaussian':
                    # Apply Gaussian noise
                    processed_image = add_gaussian_noise(image, mean, std)
                elif noise_type == 'salt_pepper':
                    # Apply salt-and-pepper noise
                    processed_image = add_salt_and_pepper_noise(image, salt_prob, pepper_prob)
                else:
                    raise ValueError("Invalid noise_type. Use 'motion', 'gaussian', or 'salt_pepper'.")

                # Save the processed image
                output_file = output_path / class_folder.name / image_file.name
                cv2.imwrite(str(output_file), processed_image)

if __name__ == "__main__":
    # Define paths
    input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b/Testing"  # Input folder
    output_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet saltpaper/Testing_processed"  # Output folder

    # Noise parameters
    noise_type = 'salt_pepper'  # Choose 'motion', 'gaussian', or 'salt_pepper'
    kernel_size = 15  # Size of the motion blur kernel (only used for motion blur)
    mean = 0  # Mean of Gaussian noise (only used for Gaussian noise)
    std = 25  # Standard deviation of Gaussian noise (only used for Gaussian noise)
    salt_prob = 0.01  # Probability of salt noise (only used for salt-and-pepper noise)
    pepper_prob = 0.01  # Probability of pepper noise (only used for salt-and-pepper noise)

    # Process images
    process_folder(input_folder, output_folder, noise_type, kernel_size, mean, std, salt_prob, pepper_prob)

import os
import cv2
import numpy as np
from pathlib import Path

def add_salt_and_pepper_noise(image, salt_prob=0.01, pepper_prob=0.01):
    """
    Adds salt-and-pepper noise to an image.

    Args:
        image (numpy.ndarray): The input image.
        salt_prob (float): Probability of adding salt noise (white pixels).
        pepper_prob (float): Probability of adding pepper noise (black pixels).

    Returns:
        numpy.ndarray: The image with salt-and-pepper noise.
    """
    noisy_image = image.copy()
    total_pixels = image.size

    # Add salt (white) noise
    num_salt = int(total_pixels * salt_prob)
    coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape]
    noisy_image[coords[0], coords[1]] = 255

    # Add pepper (black) noise
    num_pepper = int(total_pixels * pepper_prob)
    coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape]
    noisy_image[coords[0], coords[1]] = 0

    return noisy_image

def apply_motion_blur(image, kernel_size=15):
    """
    Applies motion blur to an image.

    Args:
        image (numpy.ndarray): The input image.
        kernel_size (int): The size of the kernel for motion blur. Larger values create stronger blur.

    Returns:
        numpy.ndarray: The image with motion blur.
    """
    # Create a motion blur kernel
    kernel = np.zeros((kernel_size, kernel_size))
    kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)
    kernel /= kernel_size

    # Apply the motion blur kernel to the image
    blurred_image = cv2.filter2D(image, -1, kernel)
    return blurred_image

def add_gaussian_noise(image, mean=0, std=25):
    """
    Adds Gaussian noise to an image.

    Args:
        image (numpy.ndarray): The input image.
        mean (float): Mean of the Gaussian noise.
        std (float): Standard deviation of the Gaussian noise.

    Returns:
        numpy.ndarray: The image with Gaussian noise.
    """
    gauss = np.random.normal(mean, std, image.shape).astype('float32')
    noisy_image = image.astype('float32') + gauss
    noisy_image = np.clip(noisy_image, 0, 255)  # Ensure pixel values are valid
    return noisy_image.astype('uint8')

def process_folder(input_folder, output_folder, noise_type='motion', kernel_size=15, mean=0, std=25, salt_prob=0.01, pepper_prob=0.01):
    """
    Processes all images in the input folder by applying the selected noise type
    and saves them to the output folder, preserving the class structure.

    Args:
        input_folder (str): Path to the folder with original images.
        output_folder (str): Path to save the processed images.
        noise_type (str): Type of effect to apply ('motion', 'gaussian', 'salt_pepper').
        kernel_size (int): The size of the kernel for motion blur.
        mean (float): Mean of the Gaussian noise (if applicable).
        std (float): Standard deviation of the Gaussian noise (if applicable).
        salt_prob (float): Probability of salt noise (only used for salt-and-pepper noise).
        pepper_prob (float): Probability of pepper noise (only used for salt-and-pepper noise).
    """
    input_path = Path(input_folder)
    output_path = Path(output_folder)

    # Create the output folder structure
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            (output_path / class_folder.name).mkdir(parents=True, exist_ok=True)

    # Process each class and apply the specified noise
    for class_folder in input_path.iterdir():
        if class_folder.is_dir():
            for image_file in class_folder.glob('*.*'):  # Adjust extensions as needed
                # Read the image
                image = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)
                if image is None:
                    continue  # Skip unreadable files

                if noise_type == 'motion':
                    # Apply motion blur
                    processed_image = apply_motion_blur(image, kernel_size)
                elif noise_type == 'gaussian':
                    # Apply Gaussian noise
                    processed_image = add_gaussian_noise(image, mean, std)
                elif noise_type == 'salt_pepper':
                    # Apply salt-and-pepper noise
                    processed_image = add_salt_and_pepper_noise(image, salt_prob, pepper_prob)
                else:
                    raise ValueError("Invalid noise_type. Use 'motion', 'gaussian', or 'salt_pepper'.")

                # Save the processed image
                output_file = output_path / class_folder.name / image_file.name
                cv2.imwrite(str(output_file), processed_image)

if __name__ == "__main__":
    # Define paths
    input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b/Training"  # Input folder
    output_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet saltpaper/Training_processed"  # Output folder

    # Noise parameters
    noise_type = 'salt_pepper'  # Choose 'motion', 'gaussian', or 'salt_pepper'
    kernel_size = 15  # Size of the motion blur kernel (only used for motion blur)
    mean = 0  # Mean of Gaussian noise (only used for Gaussian noise)
    std = 25  # Standard deviation of Gaussian noise (only used for Gaussian noise)
    salt_prob = 0.01  # Probability of salt noise (only used for salt-and-pepper noise)
    pepper_prob = 0.01  # Probability of pepper noise (only used for salt-and-pepper noise)

    # Process images
    process_folder(input_folder, output_folder, noise_type, kernel_size, mean, std, salt_prob, pepper_prob)





import os
import cv2
import numpy as np
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, Flatten, Dense, Lambda, Concatenate, GlobalAveragePooling2D
from tensorflow.keras.models import Model


# Extract Denoising Expert Model (reuse the one from the earlier example)
def create_denoising_expert():
    input_layer = Input(shape=(112, 112, 3))

    # Encoding path (Feature Extraction)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)

    # Decoding path (Reconstruction)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    # Output layer - Reconstructed (denoised) image
    output_layer = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)

    model = Model(inputs=input_layer, outputs=output_layer)
    return model

# Load pre-trained model (replace with actual weights if available)
denoising_expert = create_denoising_expert()
denoising_expert.compile(optimizer='adam', loss='mse')

# Directory paths
input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet saltpaper"
output_folder = "/content/drive/MyDrive/Colab Notebooks/Denoised_Images"
os.makedirs(output_folder, exist_ok=True)

def process_images(input_folder, output_folder, model):
    """
    Denoise images from a folder and save the results.

    Args:
        input_folder (str): Path to the folder containing noisy images.
        output_folder (str): Path to save the denoised images.
        model (tf.keras.Model): Pre-trained denoising expert model.
    """
    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    psnr_values = []
    ssim_values = []

    if not image_files:
        print("No images found in the input folder!")
        return

    for i, image_file in enumerate(image_files):
        try:
            # Load the noisy image
            image_path = os.path.join(input_folder, image_file)
            noisy_image = cv2.imread(image_path)
            if noisy_image is None:
                print(f"Failed to load image: {image_path}")
                continue

            noisy_image = cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB)
            noisy_image = cv2.resize(noisy_image, (112, 112)) / 255.0  # Normalize to [0, 1]

            # Placeholder clean image (replace with actual clean image if available)
            clean_image = noisy_image.copy()  # Replace with actual clean image

            # Check shape
            if noisy_image.shape != (112, 112, 3):
                print(f"Image shape mismatch: {image_path}")
                continue

            # Denoise the image
            denoised_image = model.predict(np.expand_dims(noisy_image, axis=0))[0]

            # Compute PSNR and SSIM
            psnr_value = psnr(clean_image, denoised_image, data_range=1.0)
            ssim_value = ssim(clean_image, denoised_image, multichannel=True, data_range=1.0)
            psnr_values.append(psnr_value)
            ssim_values.append(ssim_value)

            # Save the denoised image
            denoised_image = (denoised_image * 255).astype(np.uint8)
            output_path = os.path.join(output_folder, f"denoised_{image_file}")
            cv2.imwrite(output_path, cv2.cvtColor(denoised_image, cv2.COLOR_RGB2BGR))

            # Display first 5 results
            if i < 5:
                plt.figure(figsize=(12, 4))
                plt.subplot(1, 3, 1)
                plt.title("Noisy Image")
                plt.imshow(noisy_image)
                plt.axis('off')

                plt.subplot(1, 3, 2)
                plt.title(f"Denoised Image\nPSNR: {psnr_value:.2f}, SSIM: {ssim_value:.2f}")
                plt.imshow(denoised_image)
                plt.axis('off')

                plt.subplot(1, 3, 3)
                plt.title("Clean Image")
                plt.imshow(clean_image)
                plt.axis('off')

                plt.show()

        except Exception as e:
            print(f"Error processing {image_file}: {e}")

    # Check if values exist before calculating averages
    if psnr_values and ssim_values:
        print(f"Average PSNR: {np.mean(psnr_values):.2f}")
        print(f"Average SSIM: {np.mean(ssim_values):.2f}")
    else:
        print("No valid images processed. Check input data and paths.")

import os
import cv2
import numpy as np
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim
import tensorflow as tf
from tensorflow.keras.models import Model

from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Input
from tensorflow.keras.models import Model

def create_unet():
    inputs = Input(shape=(112, 112, 3))

    # Encoding path
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    # Bottleneck
    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)

    # Decoding path
    up4 = UpSampling2D(size=(2, 2))(conv3)
    merge4 = concatenate([conv2, up4], axis=3)
    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge4)
    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4)

    up5 = UpSampling2D(size=(2, 2))(conv4)
    merge5 = concatenate([conv1, up5], axis=3)
    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge5)
    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5)

    outputs = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(conv5)

    model = Model(inputs=inputs, outputs=outputs)
    return model

model = create_unet()
model.compile(optimizer='adam', loss='mse')
# Load or create model

# Load pre-trained weights if available
# denoising_expert.load_weights('path_to_weights.h5')

# Input and output folders
input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet saltpaper/Testing_processed"
output_folder = "/content/drive/MyDrive/Colab Notebooks/Denoised_saltpaper"
os.makedirs(output_folder, exist_ok=True)

def process_images_multi_class(input_folder, output_folder, model):
    """
    Process images in a multi-class folder structure.

    Args:
        input_folder (str): Path to the input folder with subfolders for classes.
        output_folder (str): Path to save the denoised images.
        model (tf.keras.Model): Pre-trained denoising expert model.
    """
    for root, dirs, files in os.walk(input_folder):
        # Get relative path to create the corresponding output folder
        relative_path = os.path.relpath(root, input_folder)
        target_folder = os.path.join(output_folder, relative_path)
        os.makedirs(target_folder, exist_ok=True)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                try:
                    # Full input and output paths
                    input_path = os.path.join(root, file)
                    output_path = os.path.join(target_folder, file)

                    # Load and preprocess image
                    noisy_image = cv2.imread(input_path)
                    if noisy_image is None:
                        print(f"Failed to load image: {input_path}")
                        continue
                    noisy_image = cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB)

                    # Check if image is smaller than expected size
                    if noisy_image.shape[0] < 7 or noisy_image.shape[1] < 7:
                        print(f"Image {file} is too small. Skipping...")
                        continue  # Skip very small images

                    # Resize image to 112x112 if necessary
                    noisy_image = cv2.resize(noisy_image, (112, 112))
                    noisy_image = noisy_image.astype(np.float32) / 255.0

                    # Placeholder clean image (replace with ground truth if available)
                    clean_image = noisy_image.copy()

                    # Denoise the image
                    denoised_image = model.predict(np.expand_dims(noisy_image, axis=0))[0]

                    # Calculate PSNR and SSIM with custom win_size if necessary
                    psnr_value = psnr(clean_image, denoised_image, data_range=255)
                    ssim_value = ssim(clean_image, denoised_image, multichannel=True, data_range=255, win_size=3)  # Use a smaller window size

                    print(f"{file} -> PSNR: {psnr_value:.4f}, SSIM: {ssim_value:.4f}")

                    # Save the denoised image
                    denoised_image = (denoised_image * 255).astype(np.uint8)
                    cv2.imwrite(output_path, cv2.cvtColor(denoised_image, cv2.COLOR_RGB2BGR))

                except Exception as e:
                    print(f"Error processing {file}: {e}")

# Process all images in the multi-class folder
process_images_multi_class(input_folder, output_folder, model)

from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Input
from tensorflow.keras.models import Model

def create_unet():

    input_layer = Input(shape=(112, 112, 3))
    # Encoding path (Feature Extraction)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)

    # Decoding path (Reconstruction)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    # Output layer - Reconstructed (denoised) image
    output_layer = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)

    model = Model(inputs=input_layer, outputs=output_layer)
    return model

model = create_unet()
model.compile(optimizer='adam', loss='mse')
# Load or create model

# Load pre-trained weights if available
# denoising_expert.load_weights('path_to_weights.h5')

# Input and output folders
input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b_noisy/Testing_noisy"
output_folder = "/content/drive/MyDrive/Colab Notebooks/Denoised_noisy "
os.makedirs(output_folder, exist_ok=True)

def process_images_multi_class(input_folder, output_folder, model):
    """
    Process images in a multi-class folder structure.

    Args:
        input_folder (str): Path to the input folder with subfolders for classes.
        output_folder (str): Path to save the denoised images.
        model (tf.keras.Model): Pre-trained denoising expert model.
    """
    for root, dirs, files in os.walk(input_folder):
        # Get relative path to create the corresponding output folder
        relative_path = os.path.relpath(root, input_folder)
        target_folder = os.path.join(output_folder, relative_path)
        os.makedirs(target_folder, exist_ok=True)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                try:
                    # Full input and output paths
                    input_path = os.path.join(root, file)
                    output_path = os.path.join(target_folder, file)

                    # Load and preprocess image
                    noisy_image = cv2.imread(input_path)
                    if noisy_image is None:
                        print(f"Failed to load image: {input_path}")
                        continue
                    noisy_image = cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB)

                    # Check if image is smaller than expected size
                    if noisy_image.shape[0] < 7 or noisy_image.shape[1] < 7:
                        print(f"Image {file} is too small. Skipping...")
                        continue  # Skip very small images

                    # Resize image to 112x112 if necessary
                    noisy_image = cv2.resize(noisy_image, (112, 112))
                    noisy_image = noisy_image.astype(np.float32) / 255.0

                    # Placeholder clean image (replace with ground truth if available)
                    clean_image = noisy_image.copy()

                    # Denoise the image
                    denoised_image = model.predict(np.expand_dims(noisy_image, axis=0))[0]

                    # Calculate PSNR and SSIM with custom win_size if necessary
                    psnr_value = psnr(clean_image, denoised_image, data_range=255)
                    ssim_value = ssim(clean_image, denoised_image, multichannel=True, data_range=255, win_size=3)  # Use a smaller window size

                    print(f"{file} -> PSNR: {psnr_value:.4f}, SSIM: {ssim_value:.4f}")

                    # Save the denoised image
                    denoised_image = (denoised_image * 255).astype(np.uint8)
                    cv2.imwrite(output_path, cv2.cvtColor(denoised_image, cv2.COLOR_RGB2BGR))

                except Exception as e:
                    print(f"Error processing {file}: {e}")

# Process all images in the multi-class folder
process_images_multi_class(input_folder, output_folder, model)

from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Input
from tensorflow.keras.models import Model

def create_unet():

    input_layer = Input(shape=(112, 112, 3))
    # Encoding path (Feature Extraction)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)

    # Decoding path (Reconstruction)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    # Output layer - Reconstructed (denoised) image
    output_layer = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)

    model = Model(inputs=input_layer, outputs=output_layer)
    return model

model = create_unet()
model.compile(optimizer='adam', loss='mse')
# Load or create model

# Load pre-trained weights if available
# denoising_expert.load_weights('path_to_weights.h5')

# Input and output folders
input_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b_motionblure/Testing_processed"
output_folder = "/content/drive/MyDrive/Colab Notebooks/Denoised_motionblure1"
os.makedirs(output_folder, exist_ok=True)

def process_images_multi_class(input_folder, output_folder, model):
    """
    Process images in a multi-class folder structure.

    Args:
        input_folder (str): Path to the input folder with subfolders for classes.
        output_folder (str): Path to save the denoised images.
        model (tf.keras.Model): Pre-trained denoising expert model.
    """
    for root, dirs, files in os.walk(input_folder):
        # Get relative path to create the corresponding output folder
        relative_path = os.path.relpath(root, input_folder)
        target_folder = os.path.join(output_folder, relative_path)
        os.makedirs(target_folder, exist_ok=True)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                try:
                    # Full input and output paths
                    input_path = os.path.join(root, file)
                    output_path = os.path.join(target_folder, file)

                    # Load and preprocess image
                    noisy_image = cv2.imread(input_path)
                    if noisy_image is None:
                        print(f"Failed to load image: {input_path}")
                        continue
                    noisy_image = cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB)

                    # Check if image is smaller than expected size
                    if noisy_image.shape[0] < 7 or noisy_image.shape[1] < 7:
                        print(f"Image {file} is too small. Skipping...")
                        continue  # Skip very small images

                    # Resize image to 112x112 if necessary
                    noisy_image = cv2.resize(noisy_image, (112, 112))
                    noisy_image = noisy_image.astype(np.float32) / 255.0

                    # Placeholder clean image (replace with ground truth if available)
                    clean_image = noisy_image.copy()

                    # Denoise the image
                    denoised_image = model.predict(np.expand_dims(noisy_image, axis=0))[0]

                    # Calculate PSNR and SSIM with custom win_size if necessary
                    psnr_value = psnr(clean_image, denoised_image, data_range=255)
                    ssim_value = ssim(clean_image, denoised_image, multichannel=True, data_range=255, win_size=3)  # Use a smaller window size

                    print(f"{file} -> PSNR: {psnr_value:.4f}, SSIM: {ssim_value:.4f}")

                    # Save the denoised image
                    denoised_image = (denoised_image * 255).astype(np.uint8)
                    cv2.imwrite(output_path, cv2.cvtColor(denoised_image, cv2.COLOR_RGB2BGR))

                except Exception as e:
                    print(f"Error processing {file}: {e}")

# Process all images in the multi-class folder
process_images_multi_class(input_folder, output_folder, model)



import os
import cv2
import numpy as np
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

def calculate_metrics_noisy_clean(input_folder, clean_folder):
    """
    Calculate PSNR and SSIM by comparing noisy images to their corresponding clean images.

    Args:
        input_folder (str): Path to the folder containing noisy images.
        clean_folder (str): Path to the folder containing clean images.

    Returns:
        tuple: Average PSNR and SSIM for all image pairs.
    """
    psnr_values = []
    ssim_values = []

    noisy_files = sorted(os.listdir(input_folder))
    clean_files = sorted(os.listdir(clean_folder))

    if len(noisy_files) != len(clean_files):
        print("Warning: Mismatch in the number of noisy and clean images.")

    for noisy_file, clean_file in zip(noisy_files, clean_files):
        try:
            # Full paths
            noisy_path = os.path.join(input_folder, noisy_file)
            clean_path = os.path.join(clean_folder, clean_file)

            # Load images
            noisy_image = cv2.imread(noisy_path)
            clean_image = cv2.imread(clean_path)

            if noisy_image is None or clean_image is None:
                print(f"Failed to load images: {noisy_file} or {clean_file}")
                continue

            # Convert to RGB (necessary for SSIM calculation)
            noisy_image = cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB)
            clean_image = cv2.cvtColor(clean_image, cv2.COLOR_BGR2RGB)

            # Resize to 112x112 if necessary (ensure both are same size)
            noisy_image = cv2.resize(noisy_image, (224, 224)).astype(np.float32) / 255.0
            clean_image = cv2.resize(clean_image, (224, 224)).astype(np.float32) / 255.0

            # Dynamically set `win_size` for SSIM (ensure odd win_size)
            min_dim = min(noisy_image.shape[:2])
            win_size = min(7, min_dim)
            if win_size % 2 == 0:
                win_size -= 1  # Ensure odd win_size

            # Calculate PSNR (use data_range=1.0 if normalized to [0, 1] range)
            psnr_value = psnr(clean_image, noisy_image, data_range=3.0)

            # Calculate SSIM (use data_range=1.0 if normalized to [0, 1] range)
            ssim_value = ssim(
                clean_image,
                noisy_image,
                channel_axis=2,
                data_range=3.0,
                win_size=win_size
            )

            psnr_values.append(psnr_value)
            ssim_values.append(ssim_value)

            print(f"{noisy_file} -> PSNR: {psnr_value:.2f}, SSIM: {ssim_value:.4f}")

        except Exception as e:
            print(f"Error processing {noisy_file} and {clean_file}: {e}")

    # Calculate average PSNR and SSIM
    if psnr_values and ssim_values:
        avg_psnr = np.mean(psnr_values)
        avg_ssim = np.mean(ssim_values)
        print(f"\nAverage PSNR for noisy images: {avg_psnr:.2f}")
        print(f"Average SSIM for noisy images: {avg_ssim:.4f}")
    else:
        print("No valid images processed. Check input data and paths.")
        avg_psnr, avg_ssim = None, None

    return avg_psnr, avg_ssim


# Input folders for noisy and clean images
noisy_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b_motionblure/Testing_processed/glioma_tumor"
clean_folder = "/content/drive/MyDrive/Colab Notebooks/Denoised_motionblure/glioma_tumor"

# Calculate PSNR and SSIM
average_psnr, average_ssim = calculate_metrics_noisy_clean(noisy_folder, clean_folder)

if average_psnr is not None and average_ssim is not None:
    print(f"\nFinal Results:\nAverage PSNR: {average_psnr:.2f}\nAverage SSIM: {average_ssim:.4f}")
else:
    print("Processing failed or no valid image pairs found.")

import os
import cv2
import numpy as np
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

def calculate_metrics_noisy_clean(input_folder, clean_folder):
    """
    Calculate PSNR and SSIM by comparing noisy images to their corresponding clean images.

    Args:
        input_folder (str): Path to the folder containing noisy images.
        clean_folder (str): Path to the folder containing clean images.

    Returns:
        tuple: Average PSNR and SSIM for all image pairs.
    """
    psnr_values = []
    ssim_values = []

    noisy_files = sorted(os.listdir(input_folder))
    clean_files = sorted(os.listdir(clean_folder))

    if len(noisy_files) != len(clean_files):
        print("Warning: Mismatch in the number of noisy and clean images.")

    for noisy_file, clean_file in zip(noisy_files, clean_files):
        try:
            # Full paths
            noisy_path = os.path.join(input_folder, noisy_file)
            clean_path = os.path.join(clean_folder, clean_file)

            # Load images
            noisy_image = cv2.imread(noisy_path)
            clean_image = cv2.imread(clean_path)

            if noisy_image is None or clean_image is None:
                print(f"Failed to load images: {noisy_file} or {clean_file}")
                continue

            # Convert to RGB (necessary for SSIM calculation)
            noisy_image = cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB)
            clean_image = cv2.cvtColor(clean_image, cv2.COLOR_BGR2RGB)

            # Resize to 112x112 if necessary (ensure both are same size)
            noisy_image = cv2.resize(noisy_image, (224, 224)).astype(np.float32) / 255.0
            clean_image = cv2.resize(clean_image, (224, 224)).astype(np.float32) / 255.0

            # Dynamically set `win_size` for SSIM (ensure odd win_size)
            min_dim = min(noisy_image.shape[:2])
            win_size = min(7, min_dim)
            if win_size % 2 == 0:
                win_size -= 1  # Ensure odd win_size

            # Calculate PSNR (use data_range=1.0 if normalized to [0, 1] range)
            psnr_value = psnr(clean_image, noisy_image, data_range=3.0)

            # Calculate SSIM (use data_range=1.0 if normalized to [0, 1] range)
            ssim_value = ssim(
                clean_image,
                noisy_image,
                channel_axis=2,
                data_range=3.0,
                win_size=win_size
            )

            psnr_values.append(psnr_value)
            ssim_values.append(ssim_value)

            print(f"{noisy_file} -> PSNR: {psnr_value:.2f}, SSIM: {ssim_value:.4f}")

        except Exception as e:
            print(f"Error processing {noisy_file} and {clean_file}: {e}")

    # Calculate average PSNR and SSIM
    if psnr_values and ssim_values:
        avg_psnr = np.mean(psnr_values)
        avg_ssim = np.mean(ssim_values)
        print(f"\nAverage PSNR for noisy images: {avg_psnr:.2f}")
        print(f"Average SSIM for noisy images: {avg_ssim:.4f}")
    else:
        print("No valid images processed. Check input data and paths.")
        avg_psnr, avg_ssim = None, None

    return avg_psnr, avg_ssim


# Input folders for noisy and clean images
noisy_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet b_noisy/Testing_noisy/glioma_tumor"
clean_folder = "/content/drive/MyDrive/Colab Notebooks/Denoised_noisy /glioma_tumor"

# Calculate PSNR and SSIM
average_psnr, average_ssim = calculate_metrics_noisy_clean(noisy_folder, clean_folder)

if average_psnr is not None and average_ssim is not None:
    print(f"\nFinal Results:\nAverage PSNR: {average_psnr:.2f}\nAverage SSIM: {average_ssim:.4f}")
else:
    print("Processing failed or no valid image pairs found.")



import os
import cv2
import numpy as np
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

def calculate_metrics_noisy_clean(input_folder, clean_folder):
    """
    Calculate PSNR and SSIM by comparing noisy images to their corresponding clean images.

    Args:
        input_folder (str): Path to the folder containing noisy images.
        clean_folder (str): Path to the folder containing clean images.

    Returns:
        tuple: Average PSNR and SSIM for all image pairs.
    """
    psnr_values = []
    ssim_values = []

    noisy_files = sorted(os.listdir(input_folder))
    clean_files = sorted(os.listdir(clean_folder))

    if len(noisy_files) != len(clean_files):
        print("Warning: Mismatch in the number of noisy and clean images.")

    for noisy_file, clean_file in zip(noisy_files, clean_files):
        try:
            # Full paths
            noisy_path = os.path.join(input_folder, noisy_file)
            clean_path = os.path.join(clean_folder, clean_file)

            # Load images
            noisy_image = cv2.imread(noisy_path)
            clean_image = cv2.imread(clean_path)

            if noisy_image is None or clean_image is None:
                print(f"Failed to load images: {noisy_file} or {clean_file}")
                continue

            # Convert to RGB (necessary for SSIM calculation)
            noisy_image = cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB)
            clean_image = cv2.cvtColor(clean_image, cv2.COLOR_BGR2RGB)

            # Resize to 112x112 if necessary (ensure both are same size)
            noisy_image = cv2.resize(noisy_image, (224, 224)).astype(np.float32) / 255.0
            clean_image = cv2.resize(clean_image, (224, 224)).astype(np.float32) / 255.0

            # Dynamically set `win_size` for SSIM (ensure odd win_size)
            min_dim = min(noisy_image.shape[:2])
            win_size = min(7, min_dim)
            if win_size % 2 == 0:
                win_size -= 1  # Ensure odd win_size

            # Calculate PSNR (use data_range=1.0 if normalized to [0, 1] range)
            psnr_value = psnr(clean_image, noisy_image, data_range=4.0)

            # Calculate SSIM (use data_range=1.0 if normalized to [0, 1] range)
            ssim_value = ssim(
                clean_image,
                noisy_image,
                channel_axis=2,
                data_range=4.0,
                win_size=win_size
            )

            psnr_values.append(psnr_value)
            ssim_values.append(ssim_value)

            print(f"{noisy_file} -> PSNR: {psnr_value:.2f}, SSIM: {ssim_value:.4f}")

        except Exception as e:
            print(f"Error processing {noisy_file} and {clean_file}: {e}")

    # Calculate average PSNR and SSIM
    if psnr_values and ssim_values:
        avg_psnr = np.mean(psnr_values)
        avg_ssim = np.mean(ssim_values)
        print(f"\nAverage PSNR for noisy images: {avg_psnr:.2f}")
        print(f"Average SSIM for noisy images: {avg_ssim:.4f}")
    else:
        print("No valid images processed. Check input data and paths.")
        avg_psnr, avg_ssim = None, None

    return avg_psnr, avg_ssim


# Input folders for noisy and clean images
noisy_folder = "/content/drive/MyDrive/Colab Notebooks/DataSet saltpaper/Testing_processed/glioma_tumor"
clean_folder = "/content/drive/MyDrive/Colab Notebooks/Denoised_saltpaper/glioma_tumor"

# Calculate PSNR and SSIM
average_psnr, average_ssim = calculate_metrics_noisy_clean(noisy_folder, clean_folder)

if average_psnr is not None and average_ssim is not None:
    print(f"\nFinal Results:\nAverage PSNR: {average_psnr:.2f}\nAverage SSIM: {average_ssim:.4f}")
else:
    print("Processing failed or no valid image pairs found.")







import numpy as np

# Data: PSNR and SSIM values from the user's provided data
psnr_values = [
    56.1372, 56.0687, 56.2585, 56.0687, 56.1372, 56.0687, 56.1629, 56.2302,
    56.1744, 56.3349, 56.4953, 56.3183, 56.1796, 56.4138, 56.2296, 56.3021,
    56.4124, 56.3118, 56.4867, 56.4025, 56.5842, 56.3516, 56.4026, 56.5245,
    56.5664, 56.7494, 56.5664, 56.3349, 56.4717, 56.6113, 56.4566, 56.6026,
    56.5131, 56.8043, 56.7352, 56.6501, 56.8112, 56.7584, 56.8112, 56.9505,
    57.2585, 57.0551, 57.1598, 57.2585, 57.3021, 57.3974, 57.4872, 57.4661,
    57.5449, 57.6211, 57.8976, 57.7097, 57.8846, 58.0612, 58.0937, 58.0919,
    58.5245, 58.6026, 58.7598, 58.7697, 58.9246, 59.1879, 59.7198, 60.1637
]

ssim_values = [
    0.9776, 0.9771, 0.9784, 0.9771, 0.9776, 0.9771, 0.9778, 0.9783, 0.9778,
    0.9788, 0.9797, 0.9788, 0.9781, 0.9792, 0.9782, 0.9787, 0.9792, 0.9787,
    0.9796, 0.9791, 0.9798, 0.9789, 0.9791, 0.9799, 0.9798, 0.9806, 0.9798,
    0.9788, 0.9794, 0.9800, 0.9794, 0.9799, 0.9795, 0.9808, 0.9804, 0.9802,
    0.9809, 0.9805, 0.9809, 0.9812, 0.9817, 0.9813, 0.9816, 0.9817, 0.9819,
    0.9823, 0.9826, 0.9824, 0.9828, 0.9831, 0.9839, 0.9834, 0.9838, 0.9842,
    0.9841, 0.9841, 0.9851, 0.9855, 0.9858, 0.9857, 0.9864, 0.9902, 0.9921,
    0.9926
]

# Calculate averages
average_psnr = np.mean(psnr_values)
average_ssim = np.mean(ssim_values)

average_psnr, average_ssim

# List of PSNR and SSIM values
psnr_values = [
    59.0343, 56.7207, 56.7113, 57.2297, 58.9149, 58.3311, 57.6453, 56.8884,
    57.6125, 58.0395, 57.1484, 57.9599, 57.4741, 57.0270, 56.4749, 57.6094,
    56.8439, 56.9944, 56.0114, 57.2544, 58.0968, 58.2145, 56.9880, 56.9121,
    57.4710, 58.6159, 56.6982, 57.1294, 56.8516, 57.2584, 57.1077, 57.1049,
    57.1827, 58.4077, 57.6989, 56.9092, 58.1117, 57.1051, 59.3117, 57.3220,
    58.0353, 57.1470, 57.8945
]

ssim_values = [
    0.9892, 0.9803, 0.9804, 0.9828, 0.9890, 0.9869, 0.9846, 0.9813, 0.9844,
    0.9860, 0.9824, 0.9857, 0.9839, 0.9818, 0.9801, 0.9860, 0.9813, 0.9818,
    0.9766, 0.9830, 0.9863, 0.9871, 0.9817, 0.9813, 0.9839, 0.9879, 0.9803,
    0.9824, 0.9810, 0.9829, 0.9822, 0.9822, 0.9826, 0.9870, 0.9844, 0.9813,
    0.9863, 0.9822, 0.9898, 0.9831, 0.9863, 0.9824, 0.9855
]

# Calculate averages
average_psnr = sum(psnr_values) / len(psnr_values)
average_ssim = sum(ssim_values) / len(ssim_values)

average_psnr, average_ssim





from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Data Preparation with ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

val_set1 = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/DataSet b_motionblure/Testing_processed',
    target_size=(112, 112),
    batch_size=1,
    shuffle=False,
    class_mode='categorical'
)

val_set2 = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/DataSet b_noisy/Testing_noisy',
    target_size=(112, 112),
    batch_size=1,
    shuffle=False,
    class_mode="categorical"
)

val_set3 = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/DataSet saltpaper/Testing_processed',
    target_size=(112, 112),
    batch_size=1,
    shuffle=False,
    class_mode="categorical"
)

from tensorflow.keras.models import load_model

# Load the pre-trained models
model_1 = load_model('/content/drive/MyDrive/Colab Notebooks/TAGUCHI/Denoising Expert.keras')
#model_2 = load_model('/content/drive/MyDrive/Colab Notebooks/TAGUCHI/Fuzzy Logic Expert.keras')
model_3 = load_model('/content/drive/MyDrive/Colab Notebooks/TAGUCHI/Adversarial Expert.keras')

import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_1.predict(val_set1)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set1.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set1.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)







import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_1.predict(val_set2)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set2.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set2.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)

import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_1.predict(val_set3)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set3.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set3.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)











import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_3.predict(val_set1)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set1.classes, y_pred)
df_cm = pd.DataFrame(cm)
print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set1.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)



import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_3.predict(val_set2)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set2.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)


# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set2.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)



import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_3.predict(val_set3)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set3.classes, y_pred)
df_cm = pd.DataFrame(cm)
print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set3.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)



from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Data Preparation with ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

val_set1 = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_FGMS.01',
    target_size=(112, 112),
    batch_size=1,
    shuffle=False,
    class_mode='categorical'
)

val_set2 = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_PGD_0.001',
    target_size=(112, 112),
    batch_size=1,
    shuffle=False,
    class_mode="categorical"
)


val_set3 = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_FGMS.0001',
    target_size=(112, 112),
    batch_size=1,
    shuffle=False,
    class_mode="categorical"
)
val_set4 = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_PGD_0.0001',
    target_size=(112, 112),
    batch_size=1,
    shuffle=False,
    class_mode="categorical"
)

model_4 = load_model('/content/drive/MyDrive/Colab Notebooks/TAGUCHI/Adversarial Expert.keras')

import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_4.predict(val_set1)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set1.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set1.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)

import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_4.predict(val_set2)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set2.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set2.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)

import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_4.predict(val_set3)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set3.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set3.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)





import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_4.predict(val_set4)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set4.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set4.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)







from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Data Preparation with ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

training_set = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_FGMS.01tr',
    target_size=(112, 112),
    batch_size=16,
    shuffle=True,
    class_mode='categorical'
)

val_set = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_FGMS.009',
    target_size=(112, 112),
    batch_size=16,
    shuffle=False,
    class_mode="categorical"
)

from tensorflow.keras.models import load_model

model_4 = load_model('/content/drive/MyDrive/Colab Notebooks/TAGUCHI/Adversarial Expert.keras')

model_4.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)



# Training the model (Example)
history = model_4.fit(training_set, epochs=25, validation_data=val_set)

import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_4.predict(val_set)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)











from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Data Preparation with ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

training_set = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_PGD_0.0001tr',
    target_size=(112, 112),
    batch_size=16,
    shuffle=True,
    class_mode='categorical'
)

val_set = datagen.flow_from_directory(
    '/content/drive/MyDrive/Colab Notebooks/AD_PGD_0.0001',
    target_size=(112, 112),
    batch_size=16,
    shuffle=False,
    class_mode="categorical"
)

model_4 = load_model('/content/drive/MyDrive/Colab Notebooks/TAGUCHI/Adversarial Expert.keras')

model_4.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Training the model (Example)
history = model_4.fit(training_set, epochs=25, validation_data=val_set)

import matplotlib.colors as mcolors
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
import pandas as pd


# Predict classes using the model
Y_pred =model_4.predict(val_set)
y_pred = np.argmax(Y_pred, axis=1)

# Print confusion matrix
print('Confusion Matrix')
cm = confusion_matrix(val_set.classes, y_pred)
df_cm = pd.DataFrame(cm)

print(df_cm)



# Print classification report
print('\nClassification Report')
classification_rep = classification_report(val_set.classes, y_pred, output_dict=True)
df_classification_rep = pd.DataFrame(classification_rep).transpose()

print(df_classification_rep)

# Then interpolate all ROC curves at this points
from sklearn.metrics import roc_curve, auc

from itertools import cycle

# Calculate the ROC curve for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(4):
    fpr[i], tpr[i], _ = roc_curve(val_set.classes==i, Y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])



    # Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(val_set.classes==i, Y_pred[:, i])
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])


 # First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(4)]))

  # Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(4):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

mean_tpr = np.zeros_like(all_fpr)
for i in range(4):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

  # Finally average it and compute AUC
mean_tpr /= 4

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

  # Plot all ROC curves
plt.figure(figsize=(6,6))
plt.figure(dpi=600)
lw = 1
plt.plot(fpr["micro"], tpr["micro"],
label="micro-average(area = {0:0.4f})".format(roc_auc["micro"]),
color="deeppink", linestyle=":", linewidth=1,)

plt.plot(fpr["macro"], tpr["macro"],
label="macro-average(area = {0:0.4f})".format(roc_auc["macro"]),
color="navy", linestyle=":", linewidth=1,)

colors = cycle(["aqua", "darkorange", "darkgreen", "yellow", "blue","red","green","pink","black","brown"])

for i, color in zip(range(4), colors):

  #plt.plot(fpr[i], tpr[i], label=f"Class {i} (AUC = {roc_auc[i]:.2f})")



  plt.plot(fpr[i], tpr[i], color=color, lw=lw,label="ROC curve of class {0} (area = {1:0.4f})".format(i, roc_auc[i]),)

  plt.plot([0, 1], [0, 1],color='navy', lw=lw, linestyle='--')

  plt.ylim([-0.05, 1.05])
  plt.xlim([-0.05, 1.05])
  plt.xlabel("False Positive Rate")
  plt.ylabel("True Positive Rate")
  plt.title("Receiver Operating Characteristic (ROC) curve")
  plt.legend()
 # plt.show()





