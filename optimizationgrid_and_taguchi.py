# -*- coding: utf-8 -*-
"""OptimizationGrid and taguchi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EtzMP9_VPFYlvHjqOuqjhhFqhS-rzqqs
"""



import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Flatten, Input
from tensorflow.keras.models import Model
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Lambda ,UpSampling2D

datagen = datagen = ImageDataGenerator(rescale=1./255)


training_set = datagen.flow_from_directory(
      '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112,112),
        batch_size=32,
        shuffle= True,
        class_mode='categorical')

val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=16,
        shuffle= False,
        class_mode="categorical")

test_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=32,
        shuffle= False,
        class_mode="categorical")

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load your custom dataset using ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

training_set = datagen.flow_from_directory(
      '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112,112),
        batch_size=32,
        shuffle=True,
        class_mode='categorical')

val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=16,
        shuffle=False,
        class_mode='categorical')

test_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=32,
        shuffle=False,
        class_mode='categorical')


# Manually performing Grid Search
def manual_grid_search():
    learning_rates = [0.001, 0.01, 0.1]
    batch_sizes = [16, 32, 64]
    num_layers = [2, 3, 4]

    best_accuracy = 0
    best_params = None

    # Iterate through the grid of hyperparameters
    for lr in learning_rates:
        for batch in batch_sizes:
            for layers_num in num_layers:
                # Create and train model with specified parameters
                model = create_model(input_shape=(112, 112, 3), learning_rate=lr, num_layers=layers_num)
                model.fit(training_set, epochs=50, batch_size=batch, validation_data=val_set, verbose=0)

                # Evaluate the model on the validation set
                _, acc = model.evaluate(val_set, verbose=0)
                print(f"Evaluating Model - LR: {lr}, Batch Size: {batch}, Layers: {layers_num} -> Accuracy: {acc}")

                if acc > best_accuracy:
                    best_accuracy = acc
                    best_params = (lr, batch, layers_num)

    print(f"Best accuracy: {best_accuracy}, Best Params: {best_params}")

# Run manual grid search
print("\nRunning Manual Grid Search Optimization:")
manual_grid_search()



import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Flatten, Input
from tensorflow.keras.models import Model
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Lambda ,UpSampling2D

datagen = datagen = ImageDataGenerator(rescale=1./255)


training_set = datagen.flow_from_directory(
      '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112,112),
        batch_size=32,
        shuffle= True,
        class_mode='categorical')

val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=16,
        shuffle= False,
        class_mode="categorical")

test_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=32,
        shuffle= False,
        class_mode="categorical")

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load your custom dataset using ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

training_set = datagen.flow_from_directory(
      '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112,112),
        batch_size=32,
        shuffle=True,
        class_mode='categorical')

val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=16,
        shuffle=False,
        class_mode='categorical')

test_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=32,
        shuffle=False,
        class_mode='categorical')


# Manually performing Grid Search
def manual_grid_search():
    learning_rates = [0.001, 0.01, 0.1]
    batch_sizes = [16, 32, 64]
    num_layers = [2, 3, 4]

    best_accuracy = 0
    best_params = None

    # Iterate through the grid of hyperparameters
    for lr in learning_rates:
        for batch in batch_sizes:
            for layers_num in num_layers:
                # Create and train model with specified parameters
                model = create_model(input_shape=(112, 112, 3), learning_rate=lr, num_layers=layers_num)
                model.fit(training_set, epochs=50, batch_size=batch, validation_data=val_set, verbose=0)

                # Evaluate the model on the validation set
                _, acc = model.evaluate(val_set, verbose=0)
                print(f"Evaluating Model - LR: {lr}, Batch Size: {batch}, Layers: {layers_num} -> Accuracy: {acc}")

                if acc > best_accuracy:
                    best_accuracy = acc
                    best_params = (lr, batch, layers_num)

    print(f"Best accuracy: {best_accuracy}, Best Params: {best_params}")

# Run manual grid search
print("\nRunning Manual Grid Search Optimization:")
manual_grid_search()

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from skopt.space import Real, Integer
from sklearn.metrics import accuracy_score

# Load your custom dataset using ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)

training_set = datagen.flow_from_directory(
      '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112,112),
        batch_size=32,
        shuffle=True,
        class_mode='categorical')

val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=16,
        shuffle=False,
        class_mode='categorical')

test_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112,112),
        batch_size=32,
        shuffle=False,
        class_mode='categorical')



def taguchi_optimization():
    # Taguchi levels for Learning rate, Batch size, Number of layers (factor values)
    learning_rates = [0.001, 0.01, 0.1]
    batch_sizes = [16, 32, 64]
    num_layers = [2, 3, 4]

    best_accuracy = 0
    best_params = None

    for lr in learning_rates:
        for batch in batch_sizes:
            for layers_num in num_layers:
                # Create and train model with specified parameters
                model = create_model(input_shape=(112,112,3), learning_rate=lr, batch_size=batch, num_layers=layers_num)
                model.fit(training_set, epochs=50, batch_size=batch, validation_data=val_set, verbose=0)
                _, acc = model.evaluate(val_set, verbose=0)

                if acc > best_accuracy:
                    best_accuracy = acc
                    best_params = (lr, batch, layers_num)

    print(f"Best accuracy: {best_accuracy}, Best Params: {best_params}")



# Comparing the techniques
print("Running Taguchi Optimization:")
taguchi_optimization()