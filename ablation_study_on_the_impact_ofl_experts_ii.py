# -*- coding: utf-8 -*-
"""Ablation Study on the Impact ofl Experts II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nh7PG2xiQscnF5q0bkebw6vJmhfzT4J4
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D, Dense, Lambda, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import time


def create_denoising_expert():
    input_layer = Input(shape=(112, 112, 3))  # Input noisy image

    # Encoding path (Feature Extraction)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)  # Down-sampling
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)  # Down-sampling

    # Decoding path (Reconstruction)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)  # Up-sampling
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)  # Up-sampling

    # Output layer - Reconstructed (denoised) image
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)

    # Flatten the feature maps to pass to the gating network
    x = GlobalAveragePooling2D()(x)

    # Build model
    model = Model(inputs=input_layer, outputs=x)
    return model


    # Define Adversarially Trained Expert Model
def create_adversarially_trained_expert():
    input_layer = Input(shape=(112, 112, 3))
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)

    x = GlobalAveragePooling2D()(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

# Define Gating Network Model
def create_gating_network(input_shape):
    input_layer = Input(shape=input_shape)
    x = Dense(512, activation='relu')(input_layer)
    x = Dense(4, activation='softmax')(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

# Combine all models
def create_combined_model(input_shape):
    # Create each model
    denoising_expert = create_denoising_expert()
    #fuzzy_logic_expert = create_fuzzy_logic_expert()
    adversarially_trained_expert = create_adversarially_trained_expert()

    # Define input layer
    input_layer = Input(shape=input_shape)

    # Get features from the denoising expert
    denoised_features = denoising_expert(input_layer)

    # Extract features from each expert
    #fuzzy_features = fuzzy_logic_expert(input_layer)
    adversarial_features = adversarially_trained_expert(input_layer)

    # Concatenate features from all experts
    combined_features = Concatenate()([denoised_features, adversarial_features])

    # Define the gating network
    gating_network = create_gating_network(combined_features.shape[1:])

    # Final prediction
    final_output = gating_network(combined_features)

    # Create and compile the combined model
    model = Model(inputs=input_layer, outputs=final_output)
    return model

# Function to run an experiment with given parameters
def run_experiment_with_model(learning_rate, batch_size, epochs=50):
    # Prepare data generators
    datagen = ImageDataGenerator(rescale=1./255)

    training_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112, 112),
        batch_size=batch_size,
        shuffle=True,
        class_mode='categorical'
    )

    val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112, 112),
        batch_size=batch_size,
        shuffle=False,
        class_mode="categorical"
    )

    # Create the combined model
    combined_model = create_combined_model((112, 112, 3))

    # Compile the model
    combined_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])

    # Time the model training
    t0 = time.time()
    r1 = combined_model.fit(training_set, epochs=epochs, validation_data=val_set, batch_size=batch_size)
    t1 = time.time()

    # Return the best validation accuracy and training time
    return max(r1.history['val_accuracy']), t1 - t0, combined_model, val_set

# Function to add Gaussian noise
def add_gaussian_noise(images, mean=0.0, stddev=0.1):
    noise = np.random.normal(mean, stddev, images.shape)
    noisy_images = np.clip(images + noise, 0, 1)  # Ensure pixel values remain between 0 and 1
    return noisy_images

# Function to evaluate the model with noisy input
def evaluate_model_with_noise(model, val_set):
    # Get validation images and labels
    images, labels = next(val_set)

    # Add noise to the validation images
    noisy_images = add_gaussian_noise(images)

    # Evaluate the model on the noisy images
    loss, accuracy = model.evaluate(noisy_images, labels, verbose=0)

    return accuracy

# Set learning rate and batch size
learning_rate = 0.001
batch_size = 32
epochs = 50

# Run the experiment
accuracy, training_time, best_model, val_set = run_experiment_with_model(learning_rate, batch_size, epochs)

# Display results
print(f"Validation Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds")

# Evaluate the best model with noisy inputs
accuracy_noisy = evaluate_model_with_noise(best_model, val_set)
print(f"Validation Accuracy with Noisy Input: {accuracy_noisy:.4f}")



import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D, Dense, Lambda, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import time



# Define Fuzzy Logic Expert Model
def fuzzy_membership(x, a, b, c):
    return tf.maximum(0.0, tf.minimum((x - a) / (b - a), (c - x) / (c - b)))

def fuzzify_layer(input_layer):
    fuzzified = Lambda(lambda x: fuzzy_membership(x, 0.2, 0.5, 0.8))(input_layer)
    return fuzzified

def create_fuzzy_logic_expert():
    input_layer = Input(shape=(112, 112, 3))

    # Fuzzification of the input data
    fuzzified_input = fuzzify_layer(input_layer)

    # Regular CNN layers
    x = Conv2D(32, (5, 5), activation='relu', padding='same')(fuzzified_input)
    x = BatchNormalization()(x)
    x = Conv2D(64, (5, 5), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)

    # Flatten and output layers
    x = GlobalAveragePooling2D()(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

# Define Adversarially Trained Expert Model
def create_adversarially_trained_expert():
    input_layer = Input(shape=(112, 112, 3))
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)

    x = GlobalAveragePooling2D()(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

# Define Gating Network Model
def create_gating_network(input_shape):
    input_layer = Input(shape=input_shape)
    x = Dense(512, activation='relu')(input_layer)
    x = Dense(4, activation='softmax')(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

# Combine all models
def create_combined_model(input_shape):
    # Create each model
    #denoising_expert = create_denoising_expert()
    fuzzy_logic_expert = create_fuzzy_logic_expert()
    adversarially_trained_expert = create_adversarially_trained_expert()

    # Define input layer
    input_layer = Input(shape=input_shape)

    # Get features from the denoising expert
    #denoised_features = denoising_expert(input_layer)

    # Extract features from each expert
    fuzzy_features = fuzzy_logic_expert(input_layer)
    adversarial_features = adversarially_trained_expert(input_layer)

    # Concatenate features from all experts
    combined_features = Concatenate()([fuzzy_features, adversarial_features])

    # Define the gating network
    gating_network = create_gating_network(combined_features.shape[1:])

    # Final prediction
    final_output = gating_network(combined_features)

    # Create and compile the combined model
    model = Model(inputs=input_layer, outputs=final_output)
    return model

# Function to run an experiment with given parameters
def run_experiment_with_model(learning_rate, batch_size, epochs=50):
    # Prepare data generators
    datagen = ImageDataGenerator(rescale=1./255)

    training_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112, 112),
        batch_size=batch_size,
        shuffle=True,
        class_mode='categorical'
    )

    val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112, 112),
        batch_size=batch_size,
        shuffle=False,
        class_mode="categorical"
    )

    # Create the combined model
    combined_model = create_combined_model((112, 112, 3))

    # Compile the model
    combined_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])

    # Time the model training
    t0 = time.time()
    r1 = combined_model.fit(training_set, epochs=epochs, validation_data=val_set, batch_size=batch_size)
    t1 = time.time()

    # Return the best validation accuracy and training time
    return max(r1.history['val_accuracy']), t1 - t0, combined_model, val_set

# Function to add Gaussian noise
def add_gaussian_noise(images, mean=0.0, stddev=0.1):
    noise = np.random.normal(mean, stddev, images.shape)
    noisy_images = np.clip(images + noise, 0, 1)  # Ensure pixel values remain between 0 and 1
    return noisy_images

# Function to evaluate the model with noisy input
def evaluate_model_with_noise(model, val_set):
    # Get validation images and labels
    images, labels = next(val_set)

    # Add noise to the validation images
    noisy_images = add_gaussian_noise(images)

    # Evaluate the model on the noisy images
    loss, accuracy = model.evaluate(noisy_images, labels, verbose=0)

    return accuracy

# Set learning rate and batch size
learning_rate = 0.001
batch_size = 32
epochs = 50

# Run the experiment
accuracy, training_time, best_model, val_set = run_experiment_with_model(learning_rate, batch_size, epochs)

# Display results
print(f"Validation Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds")

# Evaluate the best model with noisy inputs
accuracy_noisy = evaluate_model_with_noise(best_model, val_set)
print(f"Validation Accuracy with Noisy Input: {accuracy_noisy:.4f}")

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D, Dense, Lambda, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import time



# Define Fuzzy Logic Expert Model
def fuzzy_membership(x, a, b, c):
    return tf.maximum(0.0, tf.minimum((x - a) / (b - a), (c - x) / (c - b)))

def fuzzify_layer(input_layer):
    fuzzified = Lambda(lambda x: fuzzy_membership(x, 0.2, 0.5, 0.8))(input_layer)
    return fuzzified

def create_fuzzy_logic_expert():
    input_layer = Input(shape=(112, 112, 3))

    # Fuzzification of the input data
    fuzzified_input = fuzzify_layer(input_layer)

    # Regular CNN layers
    x = Conv2D(32, (5, 5), activation='relu', padding='same')(fuzzified_input)
    x = BatchNormalization()(x)
    x = Conv2D(64, (5, 5), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)

    # Flatten and output layers
    x = GlobalAveragePooling2D()(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

def create_denoising_expert():
    input_layer = Input(shape=(112, 112, 3))  # Input noisy image

    # Encoding path (Feature Extraction)
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)  # Down-sampling
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)  # Down-sampling

    # Decoding path (Reconstruction)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)  # Up-sampling
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)  # Up-sampling

    # Output layer - Reconstructed (denoised) image
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)

    # Flatten the feature maps to pass to the gating network
    x = GlobalAveragePooling2D()(x)

    # Build model
    model = Model(inputs=input_layer, outputs=x)
    return model



# Define Gating Network Model
def create_gating_network(input_shape):
    input_layer = Input(shape=input_shape)
    x = Dense(512, activation='relu')(input_layer)
    x = Dense(4, activation='softmax')(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

# Combine all models
def create_combined_model(input_shape):
    # Create each model
    denoising_expert = create_denoising_expert()
    fuzzy_logic_expert = create_fuzzy_logic_expert()
    #adversarially_trained_expert = create_adversarially_trained_expert()

    # Define input layer
    input_layer = Input(shape=input_shape)

    # Get features from the denoising expert
    denoised_features = denoising_expert(input_layer)

    # Extract features from each expert
    fuzzy_features = fuzzy_logic_expert(input_layer)
    #adversarial_features = adversarially_trained_expert(input_layer)

    # Concatenate features from all experts
    combined_features = Concatenate()([denoised_features, fuzzy_features])

    # Define the gating network
    gating_network = create_gating_network(combined_features.shape[1:])

    # Final prediction
    final_output = gating_network(combined_features)

    # Create and compile the combined model
    model = Model(inputs=input_layer, outputs=final_output)
    return model

# Function to run an experiment with given parameters
def run_experiment_with_model(learning_rate, batch_size, epochs=50):
    # Prepare data generators
    datagen = ImageDataGenerator(rescale=1./255)

    training_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Training',
        target_size=(112, 112),
        batch_size=batch_size,
        shuffle=True,
        class_mode='categorical'
    )

    val_set = datagen.flow_from_directory(
        '/content/drive/MyDrive/Colab Notebooks/BT/Testing',
        target_size=(112, 112),
        batch_size=batch_size,
        shuffle=False,
        class_mode="categorical"
    )

    # Create the combined model
    combined_model = create_combined_model((112, 112, 3))

    # Compile the model
    combined_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])

    # Time the model training
    t0 = time.time()
    r1 = combined_model.fit(training_set, epochs=epochs, validation_data=val_set, batch_size=batch_size)
    t1 = time.time()

    # Return the best validation accuracy and training time
    return max(r1.history['val_accuracy']), t1 - t0, combined_model, val_set

# Function to add Gaussian noise
def add_gaussian_noise(images, mean=0.0, stddev=0.1):
    noise = np.random.normal(mean, stddev, images.shape)
    noisy_images = np.clip(images + noise, 0, 1)  # Ensure pixel values remain between 0 and 1
    return noisy_images

# Function to evaluate the model with noisy input
def evaluate_model_with_noise(model, val_set):
    # Get validation images and labels
    images, labels = next(val_set)

    # Add noise to the validation images
    noisy_images = add_gaussian_noise(images)

    # Evaluate the model on the noisy images
    loss, accuracy = model.evaluate(noisy_images, labels, verbose=0)

    return accuracy

# Set learning rate and batch size
learning_rate = 0.001
batch_size = 32
epochs = 50

# Run the experiment
accuracy, training_time, best_model, val_set = run_experiment_with_model(learning_rate, batch_size, epochs)

# Display results
print(f"Validation Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds")

# Evaluate the best model with noisy inputs
accuracy_noisy = evaluate_model_with_noise(best_model, val_set)
print(f"Validation Accuracy with Noisy Input: {accuracy_noisy:.4f}")



